{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from typing import List, Literal\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IngestionBronzeAirport:\n",
    "    \"\"\"\n",
    "    A class to load the data from airports.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    source : Literal['VRA', 'AIR_CIA', 'icaos']\n",
    "        The source of the data.\n",
    "    output_format: Literal['csv', 'parquet', 'json']\n",
    "        Format of the output file.\n",
    "    folder_path_incremental : str\n",
    "        The path of the incremental folder.\n",
    "    folder_path_history : str\n",
    "        The path of the history folder.\n",
    "    file_format : Literal['csv', 'json', 'parquet']\n",
    "        Format of the input file.\n",
    "    pks: List[str]\n",
    "        The primary keys of the table.\n",
    "    order_by: str\n",
    "        The order to filter the table.\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    get_file_paths() -> List[str]:\n",
    "        Get a list of all files in the folder with the given file_format.\n",
    "    get_df_from_csv(file_paths: List[str], **kwargs) -> List[pd.DataFrame] :\n",
    "        Load the file from a CSV file to a dataframe.\n",
    "    get_df_from_json(file_paths: List[str], **kwargs) -> List[pd.DataFrame] :\n",
    "        Load the file from a JSON file to a dataframe.\n",
    "    get_df_from_parquet(file_paths: List[str], **kwargs) -> List[pd.DataFrame] :\n",
    "        Load the file from a JSON file to a dataframe.\n",
    "    move_file(file_path: str) -> None :\n",
    "        Moves the staged file when the extraction is completed.\n",
    "    extract(file_paths: List[str], **kwargs) -> List[pd.DataFrame] :\n",
    "        Extract the data from a file and return a dataframe.\n",
    "    consolidate_dfs(self, dfs: List[pd.DataFrame]) -> pd.DataFrame :\n",
    "        Concatenate a list of dataframes into a single dataframe.\n",
    "    upsert(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        Selects only the most up to date rows from the dataframe.\n",
    "    transform(df: pd.DataFrame) -> pd.DataFrame :\n",
    "        Apply all transformations to the list of dataframes \n",
    "        and consolidates as a single one.\n",
    "    save_file(df: pd.DataFrame, source: str | None, **kwargs) -> None:\n",
    "        Saves the dataframe in the specified file format.\n",
    "    process_incremental() -> pd.DataFrame :\n",
    "        Process the data from incremental path.\n",
    "    process_history() -> pd.DataFrame :\n",
    "        Process the data from history path.\n",
    "    process_data() -> None :\n",
    "        Add the data from folder incremental with the history.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 source: Literal['VRA', 'AIR_CIA', 'icaos'],\n",
    "                 output_format: Literal['csv', 'parquet', 'json'] = 'parquet'\n",
    "                 ) -> None:\n",
    "        \"\"\"\n",
    "        Constructs all the necessary atributes for the airport data object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        source : Literal['VRA', 'AIR_CIA', 'icaos']\n",
    "            The source of the data.\n",
    "        output_format: Literal['csv', 'parquet', 'json']\n",
    "            Format of the output file.\n",
    "        \"\"\"\n",
    "\n",
    "        self.source = source\n",
    "        self.output_format = output_format\n",
    "\n",
    "        with open('config.json') as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        self.folder_path_incremental = config[source]['folder_path_incremental']\n",
    "        self.folder_path_history = config[source]['folder_path_history']\n",
    "        self.file_format = config[source]['file_format']\n",
    "        self.pks = config[source]['pks']\n",
    "        self.order_by = config[source]['order_by']\n",
    "\n",
    "\n",
    "    def get_file_paths(self, folder_path: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get a list of all files in the folder with the given file_format.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        folder_path : str\n",
    "            The path to the folder\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[str]\n",
    "            A list of strings.\n",
    "        \"\"\"\n",
    "\n",
    "        file_paths = glob.glob(os.path.join(f'{folder_path}/{self.file_format}', f\"*.{self.file_format}\"))\n",
    "        if not file_paths:\n",
    "             print(f\"No {self.file_format} files found in the folder {folder_path}\")\n",
    "        \n",
    "        return file_paths\n",
    "    \n",
    "    \n",
    "    def get_df_from_csv(self, file_paths: List[str], **kwargs) -> List[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Load the file from a CSV file to a dataframe\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_paths : List[str]\n",
    "            A list of paths to the CSV files.\n",
    "        kwargs : kwargs\n",
    "            Keyword arguments.\n",
    "        Returns\n",
    "        -------\n",
    "        List[pd.DataFrame]\n",
    "            A list of dataframes.\n",
    "        \"\"\"\n",
    "\n",
    "        return [pd.read_csv(path, **kwargs) for path in file_paths]\n",
    "        \n",
    "    \n",
    "    \n",
    "    def get_df_from_json(self, file_paths: List[str], **kwargs) -> List[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Load the file from a JSON file to a dataframe\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_paths : List[str]\n",
    "            A list of paths to the JSON files.\n",
    "        kwargs : kwargs\n",
    "            Keyword arguments.\n",
    "        Returns\n",
    "        -------\n",
    "        List[pd.DataFrame]\n",
    "            A list of dataframes.\n",
    "        \"\"\"\n",
    "\n",
    "        return [pd.read_json(path, **kwargs) for path in file_paths]\n",
    "        \n",
    "\n",
    "    def get_df_from_parquet(self, file_paths: List[str], **kwargs) -> List[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Load the file from a parquet file to a dataframe\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_paths : List[str]\n",
    "            A list of paths to the parquet files.\n",
    "        kwargs : kwargs\n",
    "            Keyword arguments.\n",
    "        Returns\n",
    "        -------\n",
    "        List[pd.DataFrame]\n",
    "            A list of dataframes.\n",
    "        \"\"\"\n",
    "\n",
    "        return [pd.read_parquet(path, **kwargs) for path in file_paths]\n",
    "    \n",
    "    \n",
    "    def move_file(self, file_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Moves the staged file when the extraction is completed.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_path : str\n",
    "            File path to the source file.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        new_path = file_path.replace('staged', 'raw')\n",
    "        folder = '/'.join(new_path.split('/')[:-1])\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        os.rename(file_path, new_path)\n",
    "    \n",
    "\n",
    "    def extract(self, file_paths: List[str], **kwargs) -> List[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extract the data from a file and return a dataframe.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_paths : List[str]\n",
    "            A list of paths to the files.\n",
    "        kwargs : dict\n",
    "            Keyword arguments.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[pd.DataFrame]\n",
    "            A list of dataframes.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        dfs = getattr(self, f'get_df_from_{self.file_format}')(file_paths, **kwargs)\n",
    "        for path in file_paths:\n",
    "            if 'staged' in path:\n",
    "                self.move_file(path)\n",
    "\n",
    "        return dfs\n",
    "    \n",
    "    \n",
    "    def consolidate_dfs(self, dfs: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Concatenate a list of dataframes into a single dataframe.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dfs : List[pd.DataFrame]\n",
    "            A list of dataframes to concatenate.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            A single dataframe.\n",
    "        \"\"\"\n",
    "\n",
    "        return pd.concat(dfs)\n",
    "    \n",
    "    def upsert(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Selects only the most up to date rows from the dataframe.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            A dataframe to be upserted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            a DataFrame with the most up to date rows.\n",
    "        \"\"\"\n",
    "\n",
    "        df = (\n",
    "                df\n",
    "                .sort_values(by=[self.order_by], ascending=False)\n",
    "                .drop_duplicates(subset=self.pks, keep='first')\n",
    "            )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "\n",
    "    def transform(self, dfs: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply all transformations to the list of dataframes \n",
    "        and consolidates as a single one.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dfs : List[pd.DataFrame]\n",
    "            A list of dataframes to transforme.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            A dataframe with all transformations applied.\n",
    "        \"\"\"\n",
    "\n",
    "        df = self.consolidate_dfs(dfs)\n",
    "        df = self.upsert(df)\n",
    "\n",
    "        return df\n",
    "        \n",
    "    \n",
    "    def save_file(self, df: pd.DataFrame, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Saves the dataframe in the specified file format.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Dataframe to be saved as a file.\n",
    "        source : str |  None (optional)\n",
    "            source of the data\n",
    "        kwargs : kwargs\n",
    "            Keyword arguments\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "            \n",
    "        if not os.path.exists(self.folder_path_history):\n",
    "            os.makedirs(self.folder_path_history)\n",
    "        \n",
    "        filepath = f'{self.folder_path_history}/bronze_{self.source}.{self.output_format}'\n",
    "\n",
    "        if self.output_format == 'json' and not kwargs:\n",
    "            kwargs = {'orient': 'records', 'indent': 4}\n",
    "            \n",
    "        getattr(df, f'to_{self.output_format}')(filepath, index=False, **kwargs)\n",
    "\n",
    "\n",
    "    def process_incremental(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process the data from incremental path.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            A dataframe with all the data from the incremental path\n",
    "        \"\"\"\n",
    "\n",
    "        file_paths_incr = self.get_file_paths(self.folder_path_incremental)\n",
    "        \n",
    "        if file_paths_incr:\n",
    "            dfs_incr = self.extract(file_paths=file_paths_incr)\n",
    "            df_incr = self.transform(dfs_incr)\n",
    "            return df_incr\n",
    "    \n",
    "\n",
    "    def process_history(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process the data from history path.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            A dataframe with all the data from the history path\n",
    "        \"\"\"\n",
    "\n",
    "        file_paths_hist = self.get_file_paths(self.folder_path_history)\n",
    "        \n",
    "        if file_paths_hist:\n",
    "            dfs_hist = self.extract(file_paths=file_paths_hist)\n",
    "            df_hist = self.transform(dfs_hist)\n",
    "            return df_hist\n",
    "    \n",
    "\n",
    "    def process_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Add the data from folder incremental with the history.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        \n",
    "        df_incr = self.process_incremental()\n",
    "        df_hist = self.process_history()\n",
    "        df = self.transform([df_incr, df_hist])\n",
    "        self.save_file(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No parquet files found in the folder /Users/USUARIO/Library/CloudStorage/OneDrive-Personal/Documentos/Casos/Eleflow/de_case_response/data/bronze/icaos\n"
     ]
    }
   ],
   "source": [
    "ing = IngestionBronzeAirport(\n",
    "        source='icaos'\n",
    "    )\n",
    "\n",
    "ing.process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No parquet files found in the folder /Users/USUARIO/Library/CloudStorage/OneDrive-Personal/Documentos/Casos/Eleflow/de_case_response/data/loaded/VRA\n",
      "No parquet files found in the folder /Users/USUARIO/Library/CloudStorage/OneDrive-Personal/Documentos/Casos/Eleflow/de_case_response/data/loaded/AIR_CIA\n",
      "No parquet files found in the folder /Users/USUARIO/Library/CloudStorage/OneDrive-Personal/Documentos/Casos/Eleflow/de_case_response/data/loaded/icaos\n"
     ]
    }
   ],
   "source": [
    "sources = ['VRA', 'AIR_CIA', 'icaos']\n",
    "\n",
    "for source in sources:\n",
    "    ing = IngestionBronzeAirport(\n",
    "        source=source\n",
    "    )\n",
    "    ing.process_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
